# coding:utf-8
import torch
from torch import nn

"""
    inference: https://arxiv.org/pdf/1903.06586.pdf
"""


class SKConv(nn.Module):
    def __init__(self, features, WH, M, G, r, L=32):
        super(SKConv, self).__init__()
        d = max(int(features / r), L)
        self.M = M
        self.features = features
        self.convs = nn.ModuleList([])
        for i in range(M):
            self.convs.append(nn.Sequential(
                nn.Conv2d(features, features, kernel_size=3 + i * 2, stride=1, padding=1 + i, groups=G),
                nn.BatchNorm2d(features),
                nn.ReLU(inplace=True)
            ))
        self.gap = nn.AvgPool2d(WH)
        self.fc = nn.Linear(features, d)
        self.fcs = nn.ModuleList([])
        for i in range(M):
            self.fcs.append(
                nn.Linear(d, features)
            )
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        for i, conv in enumerate(self.convs):
            fea = conv(x).unsqueeze_(dim=1)
            if i == 0:
                feas = fea
            else:
                feas = torch.cat([feas, fea], dim=1)
        fea_U = torch.sum(feas, dim=1)
        fea_s = self.gap(fea_U).squeeze_()
        fea_z = self.fc(fea_s)
        for i, fc in enumerate(self.fcs):
            vector = fc(fea_z).unsqueeze_(dim=1)
            if i == 0:
                attention_vectors = vector
            else:
                attention_vectors = torch.cat([attention_vectors, vector], dim=1)
        attention_vectors = self.softmax(attention_vectors)
        attention_vectors.unsqueeze_(-1).unsqueeze_(-1)
        fea_v = (feas * attention_vectors).sum(dim=1)
        return fea_v


class SKUnit(nn.Module):
    def __init__(self, in_features, out_features, WH, M, G, r, L=32):
        super(SKUnit, self).__init__()
        self.conv1 = nn.Conv2d(in_features, out_features, 1, stride=1)
        self.skConv = SKConv(in_features, WH, M, G, r, L)
        self.conv2 = nn.Conv2d(in_features, out_features, 1, stride=1)

    def forward(self, x):
        fea = self.conv1(x)
        fea = self.skConv(fea)
        fea = self.conv2(fea)
        return fea


if __name__ == '__main__':
    x = torch.rand(8, 64, 32, 32)
    conv = SKConv(64, 32, 2, 8, 2)
    out = conv(x)
    print(out.shape)